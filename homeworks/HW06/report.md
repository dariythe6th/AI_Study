# HW06 – Report

> Файл: `homeworks/HW06/report.md`  
> Важно: не меняйте названия разделов (заголовков). Заполняйте текстом и/или вставляйте результаты.

## 1. Dataset

- Какой датасет выбран: `S06-hw-dataset-03.csv`
- Размер: (15000 строк, 30 столбцов)
- Целевая переменная: `target` (3 класса: 0, 1, 2)
  - Класс 0: 54.25%
  - Класс 1: 30.23% 
  - Класс 2: 15.51%
- Признаки: 28 числовых признаков (f01-f28, тип float64), 1 технический столбец id (int64), целевая переменная target (int64). Категориальных признаков нет, все признаки числовые.

## 2. Protocol

- Разбиение: train/test (80/20, `random_state=42`, `stratify=y` для сохранения пропорций классов)
- Подбор: CV на train (5 фолдов для DecisionTree и RandomForest, 3 фолда для HistGradientBoosting), оптимизировали метрику F1-macro
- Метрики: 
  - **Accuracy** - общая точность классификации
  - **F1-macro** - среднее гармоническое precision и recall по всем классам, учитывает дисбаланс классов
  - **ROC-AUC OVR** (One-vs-Rest) - площадь под ROC-кривой для мультиклассовой классификации, показывает способность модели разделять классы

Эти метрики уместны, потому что:
1. Мультиклассовая классификация (3 класса) требует метрик, учитывающих все классы
2. Наблюдается дисбаланс классов (класс 0 - 54%, класс 2 - 15.5%)
3. ROC-AUC OVR показывает качество разделения классов, а F1-macro балансирует precision и recall

## 3. Models

Сравнивались следующие модели:

1. **DummyClassifier** (baseline)
   - Стратегия: `most_frequent` (предсказывает самый частый класс)

2. **LogisticRegression** (baseline из S05)
   - Pipeline: `StandardScaler` + `LogisticRegression`
   - `max_iter=1000`, `random_state=42`

3. **DecisionTreeClassifier** (контроль сложности)
   - Подбираемые параметры: `max_depth` [3, 5, 10], `min_samples_leaf` [1, 5, 10]
   - `random_state=42`

4. **RandomForestClassifier**
   - Подбираемые параметры: `n_estimators` [50, 100], `max_depth` [5, 10], `max_features` ['sqrt', 'log2']
   - `random_state=42`

5. **HistGradientBoostingClassifier** (boosting)
   - Подбираемые параметры: `max_iter` [50, 100], `learning_rate` [0.1, 0.3], `max_depth` [3, 5], `l2_regularization` [0, 0.1]
   - Дополнительные настройки: `max_bins=128`, `early_stopping=True`, `n_iter_no_change=5`
   - `random_state=42`

## 4. Results

Финальные метрики на test по всем моделям:

| Модель | Accuracy | F1-macro | ROC-AUC OVR |
|--------|----------|----------|-------------|
| Dummy | 0.5425 | 0.2088 | 0.5000 |
| LogisticRegression | 0.7237 | 0.6651 | 0.8481 |
| DecisionTree | 0.7943 | 0.7385 | 0.8597 |
| RandomForest | 0.8587 | 0.8174 | 0.9447 |
| **HistGradientBoosting** | **0.8877** | **0.8616** | **0.9543** |

**Победитель:** HistGradientBoostingClassifier

**Краткое объяснение:** HistGradientBoosting показал наилучшие результаты по всем трем метрикам. Особенно впечатляет ROC-AUC OVR = 0.9543, что свидетельствует об отличной разделительной способности модели. Модель также достигла наивысшего F1-macro (0.8616), что важно при дисбалансе классов.

## 5. Analysis

**Устойчивость:** Для проверки устойчивости был проведен дополнительный эксперимент с RandomForest (как второй лучшей модели) при разных `random_state` [42, 123, 456, 789, 999]. Результаты показали стабильность:
- Accuracy: 0.8587 ± 0.0032
- F1-macro: 0.8174 ± 0.0028
- ROC-AUC OVR: 0.9447 ± 0.0015

**Ошибки (confusion matrix для лучшей модели):**
[[1625 15 5]
[ 49 556 36]
[ 12 31 271]]


Анализ confusion matrix:
- **Класс 0:** Отлично предсказывается (1625 правильных из 1645), минимальные ошибки
- **Класс 1:** 556 правильных из 641, основная путаница с классом 2 (36 ошибок)
- **Класс 2:** 271 правильных из 314, путается с классом 1 (31 ошибка)
- **Основной вывод:** Модель лучше всего предсказывает доминирующий класс 0, хуже различает миноритарные классы 1 и 2

**Интерпретация (permutation importance, top-15):**

Топ-15 самых важных признаков:
1. f13: 0.1333
2. f28: 0.0988
3. f05: 0.0734
4. f15: 0.0580
5. f12: 0.0566
6. f10: 0.0548
7. f01: 0.0499
8. f17: 0.0484
9. f07: 0.0337
10. f06: 0.0314
11. f18: 0.0304
12. f11: 0.0224
13. f27: 0.0187
14. f22: 0.0122
15. f03: 0.0046

**Выводы:**
1. Признаки распределены неравномерно по важности - топ-3 признака дают ~30% важности
2. Наиболее информативны признаки f13, f28, f05
3. Некоторые признаки (f03, f22) имеют очень низкую важность, что может указывать на возможность их удаления

## 6. Conclusion

3-6 коротких тезисов: что вы поняли про деревья/ансамбли и про честный ML-протокол.

1. **Ансамбли превосходят одиночные модели:** RandomForest и HistGradientBoosting показали значительно лучшие результаты, чем DecisionTree, что подтверждает эффективность ансамблевых методов для снижения variance и улучшения обобщающей способности.

2. **Контроль сложности критически важен:** Для DecisionTree ограничение глубины (`max_depth=5`) и минимального количества образцов в листьях (`min_samples_leaf=10`) позволило избежать переобучения и получить разумный баланс bias-variance.

3. **Честный ML-протокол требует дисциплины:** Фиксированный train/test split с `random_state`, стратификация, подбор гиперпараметров только на train через CV, однократная оценка на test - все эти шаги обеспечивают надежную и воспроизводимую оценку моделей.

4. **Метрики должны соответствовать задаче:** Для мультиклассовой классификации с дисбалансом F1-macro и ROC-AUC OVR оказались более информативными, чем accuracy. DummyClassifier с accuracy 54% наглядно показал, почему accuracy может вводить в заблуждение.

5. **Бустинг может быть оптимальным выбором:** HistGradientBoosting показал наилучшие результаты, демонстрируя преимущество последовательного улучшения модели. При этом важно использовать регуляризацию (`l2_regularization`) и раннюю остановку для предотвращения переобучения.

6. **Интерпретируемость vs качество:** RandomForest обеспечивает хорошую интерпретируемость через feature importance при высоком качестве, в то время как HistGradientBoosting дает лучшее качество, но требует дополнительных методов интерпретации (permutation importance).
